# Hyperparameter Optimization (HPO)

In order to be able to bloom in winter, the Queen had optimized her genes in the past hundreds of years. When the cold weather comes, her body parameters can be adjusted to the best status.

Similar to the Queen, in Applied Data Science, we can adjust the hyperparameters of a model in order to get an optimal output, this process is known as ‚ÄúHyperparameter Optimization‚Äù (HPO).

<p align="center">üå± <b>Seeds Collection Time!</b> üå±</p>

The seeds in this stop has the power of appliying the latest HPO technology in classical classification & regression as well as deep learning problems, using FLAML, Optuna and Keras Tuner.

<p>&nbsp;</p>

## FLAML vs Optuna - HPO for Classical Machine learning

* Optuna is a hyperparameter optimization framework published in 2019, it was designed for improving cost effectiveness of HPO with efficient parameter searching strategy and pruning algorithm. Till 2021, it has become a mature tool that supports different frameworks of machine learning and deep learning.

* FLAML is an automated HPO library published in 2021, powered by self-invented parameter searching algorithms, it aims at freeing users from selecting learners and hyperparameters while delivering fast and economical HPO results.

While working on some garden business, Lady H. has experimented with this magical seed and took the note:





