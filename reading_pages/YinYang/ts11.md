## Outlier Detection

If we take a brief looking back, Kats trend detection and changepoint detection in both Kats and Greykite all return detected data points. Beside, any other data points that are worthy to explore?

One of an interesting concept is the outliers in time series. As you saw in [Queen Lotus][1], an outlier is an object that's quite diffrent from the rest of the data. In time series, such data point can be insightful, such as a data peak in fraud detection might tell us when there might be an abnormal event, or the skyrocketing online sales during COVID time can help us forecast customers' behavior in pandemic period better, or we saw an extremely high data volume that's caused by a system error which had been fixed and won't happen again, etc., etc., there are many other fun examples.

Some of these outliers can be ignored, some should be take into consideration of later model forecasting. Therefore, it's often a good practice to understand the causes of the outliers after the detection, then decide whether to remove the outliers.

In this section, you will not only see Kats and Greykite outlier detection, but also Lady H.'s self-implemented methods! ðŸ˜‰

### Kats Outlier Detection (Univariate Time Series)

Kats outlier detection for univariate time series is based on `n * IQR` idea, if a value is more than `n * IQR` away from Q1 (quantile 1) or Q3 (quantile 3), then this value will be considered as an outlier, you can adjust the value of `n`.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/notes/iqr.png" width="766" height="79" />
</p>

More detailed process is, Kats will decompose the time series first using additive or multiplicative decomposition, its purpose is to remove trend, seasonality and only keep residuals, then it detects points in residuals that are outside of `n * IQR`. By default, Kats uses `n=3` but you can adjust its value with parameter `iqr_mult`.

The detection code is as simple as the 2 lines of code below. The time series here is sales data which was proved to be a better fit for multiplication decomposition before, therefore here we are using "multiplication" as the decomposing method, set `iqr_mult` as 5 so that we can get more obvious outliers.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/kats_outlier_code.png" width="686" height="46" />
</p>

If we plot the detected outliers on the original timeseries data, it's more difficult to understand why they are the outliers, since the detected outliers are not all appear at crest or trough. However, after removing trend and seasonality effects, the detected residual data points do appear to be more different from the rest of the data. 

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/kats_outliers.png" width="1203" height="313" />
</p>

This also tells us, sometimes the crest or trough of a time series may not be an outlier even if they have the most extreme values, they might be caused by seasonality or trend.

At the same time, kats provides outlier removal function, and there are 2 options:
* No interpolation option will replace detected outliers with NAN
* With interpolation option will replace detected outliers using [linear interpolation][3] values

In the charts below, Lady H. has plotted the results from both options, the yellow line is the original time series while the green line is the time series after outlier detection and removal:

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/kats_outlier_removal.png" width="1203" height="313" />
</p>

Again, we need to be cautious of outliers removal in time series data, since keeping some insightful outliers for later model forecasting will bring us more benefits than removing them.

ðŸŒ» [Check detailed code in Kats Outlier Detection >>][4]


### Lady H.'s Self Implemented Outlier Detection (Multivariate Time Series)

Although Kats provides multivariate time series detection, it simply didn't work. Check [this chat][2], you will see the errors Lady H. got in Kats multivariate outlier detection, you will also find the evidence that the problem was caused by the bug in Kats. After checking the basic logic of Kats multivariate outlier detection, Lady H. realized, it's more efficient to implement the method herself than debugging Kats. Besides, she implemented more solutions for multivariate time series outlier detection! ðŸ˜‰

#### VAR for Multivariate Time Series Outlier Detection

Kats was using VAR (Vector Auto Regression) for multivariate time series' outlier detection. Lady H. made some improvement upon this idea, and the general process is:

1. Make sure every time series in the input is stationary
2. Fit VAR model with an optimal order, and apply Durbin Watson test to make sure there is leftover pattern in the residuals if any time series
3. Calculate squared errors with the fitted VAR, then calculate a threshold as `threshold = avg + n * std`
4. The outliers are data records with its squared error higher than the threshold

Now let's dive into more details!

First of all, we need to understand how does VAR work.VAR is a statistical model used for capturing the relationship between multiple quantities as they change over time. In VAR model, each variable is modeled as a linear combination of past values of itself and the past values of other variables in the system. Since you have multiple time series that influence each other, it is modeled as a system of equations with one equation per variable (time series). To help you understand its formula better, Lady H. provided 3 examples below.

* In the 1st example, there are 2 variables `Y1` and `Y2`, each variable is represented as a linear combination of its lags (past values) and other variables' lags. `Î±` is the intercept, `Î²` is the coefficient, `Îµ` is the error term. The number of lags is the order of VAR, so in this example the past values are only using `t-1`, so the order is 1.
* Similarily, in the 2nd example, the order is 2 because each variable has used `t-1`, `t-2` lags.
* In the 3rd exaample, the VAR order is still 2, but there are 3 variables `Y1`, `Y2`, `Y3`.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/var_examples.png" width="963" height="419" />
</p>

Before applying VAR, we need to convert all the input variables to stationary, this is the premise of VAR. In fact we have already done this in previous data exploration section, after doing the first order differencing on "Humidity" and "HumidityRatio", we can get each time series to be differencing stationary and trending stationary.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/stationary_multi_ts.png" width="524" height="234" />
</p>

Next, we will find the optimal order for VAR. We provide a max lag value, then VAR will select an order value that's no more than this max value and also gets best performance. VAR uses 4 model selection metrics to decide the optimal order:
* AIC (Akaike's Information Criterion) is an estimator of prediction error of any estimated statistical model. It uses a penalty term when exceeding the optimal number of parameters. AIC generally tries to find unknown model that has high dimensional reality, which also means the models are not true models in AIC.
* BIC (Bayesian Information Criteria) is a type of model selection among a class of parametric models with different numbers of parameters. Comparing with AIC, BIC only come across true models and penalizes additional parameters stronger.
* FPE (Final Prediction Error) estimates the model-fitting error when you use the model to predict new outputs. A model with a lower FPE has a better balance between the number of parameters and the explained variation.
* HQIC (Hannanâ€“Quinn information criterion) also has a penality term as AIC but the penality is stronger.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/var_opt_order.png" width="379" height="791" />
</p>

As we saw from the output above, when order is 18, it gets lowest AIC and FPE, BIC and HQIC are almost the lowest too. Therefore, the optimal order was selected as 18. But getting VAR selected optimal order is not enough, we need to ensure there is no leftover pattern in the residuals. To do this, we use Dubin Watson, it checks the serial correlation to ensure that the model is able to explain the variances and patterns in the time series sufficiently, so there there is no leftover patter in the residuals. Its output is between [0, 4], when the value is close to 2, then there is no significant serial correlation Closer to 0 indicates a positive serial correlation and closer to 4 implies a negative serial correlation. 

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/durbin_watson_test.png" width="871" height="233" />
</p>

The output above looks good. Otherwise, more actions are needed, you can increase the order of the model or bring in more variables or look for a different model.


[1]:https://github.com/lady-h-world/My_Garden/blob/main/reading_pages/the_queen.md#the-outlier
[2]:https://github.com/facebookresearch/Kats/issues/194#event-6385149398
[3]:https://en.wikipedia.org/wiki/Interpolation#Linear_interpolation
[4]:https://github.com/lady-h-world/My_Garden/blob/main/code/yinyang/kats_experiments/kats_detect_outliers.ipynb
